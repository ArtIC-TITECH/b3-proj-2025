{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee2f0504",
   "metadata": {},
   "source": [
    "# テーマB：重みのベクトル量子化とスカラー量子化の効率度比較とその融合手法の検討\n",
    "[Open In Colab](https://colab.research.google.com/github/ArtIC-TITECH/b3-proj-2025/blob/main/theme_B/theme_B.ipynb)\n",
    "\n",
    "## モジュールの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22965170",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Function\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b9de94",
   "metadata": {},
   "source": [
    "## MNISTのデータセット/精度評価関数の作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44abda6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 実行デバイスの設定\n",
    "device = 'cuda:2'\n",
    "\n",
    "# 普通のtransform\n",
    "transform_normal = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# テストデータには普通のtransformを使ってください\n",
    "transform_for_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform_normal) # モデルの学習に使うデータセット\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform_for_test) # モデルの評価に使うデータセット\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "def compute_accuracy(model, test_loader, device='cuda:0'):\n",
    "    model.eval()  # 評価モード\n",
    "    model.to(device)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images.to(device))\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels.to(device)).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy: {accuracy:.2f}%')\n",
    "    return accuracy\n",
    "\n",
    "def train(model, lr=0.05, epochs=5, device='cuda:0'):\n",
    "    # 損失関数と最適化手法の定義\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    model.to(device)\n",
    "    for epoch in range(epochs):\n",
    "        loss_sum = 0\n",
    "        for images, labels in train_loader:\n",
    "            # モデルの予測\n",
    "            outputs = model(images.to(device))\n",
    "\n",
    "            # 損失の計算\n",
    "            loss = criterion(outputs, labels.to(device))\n",
    "            loss_sum += loss.item()\n",
    "\n",
    "            # 勾配の初期化\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # バックプロパゲーション\n",
    "            loss.backward()\n",
    "\n",
    "            # オプティマイザの更新\n",
    "            optimizer.step()\n",
    "\n",
    "        # 損失を表示\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss_sum/len(train_loader):.4f}')\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16452350",
   "metadata": {},
   "source": [
    "## 通常モデルの学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08f47af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.4591\n",
      "Epoch [2/5], Loss: 0.1775\n",
      "Epoch [3/5], Loss: 0.1298\n",
      "Epoch [4/5], Loss: 0.1030\n",
      "Epoch [5/5], Loss: 0.0844\n",
      "Accuracy: 96.68%\n"
     ]
    }
   ],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self): # モデルのセットアップ\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x): # モデルが行う処理\n",
    "        x = x.view(-1, 28 * 28)  # 28x28の画像を１次元に変換\n",
    "        x = self.fc1(x) \n",
    "        x = nn.ReLU()(x) # 活性化関数\n",
    "        x = self.fc2(x) \n",
    "        x = nn.ReLU()(x) # 活性化関数\n",
    "        x = self.fc3(x) \n",
    "        return x\n",
    "\n",
    "# モデルのインスタンスを作成\n",
    "model = SimpleModel().to(device)\n",
    "\n",
    "model = train(model, lr=0.1, epochs=5, device=device)\n",
    "\n",
    "accuracy = compute_accuracy(model, test_loader, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430b5daa",
   "metadata": {},
   "source": [
    "## スカラー量子化（一様対称量子化）の実行\n",
    "\n",
    "###  プロセス：量子化層に変換-->量子化認識学習\n",
    "\n",
    "ここでは簡便に量子化パラメータをmin-maxスケーリングで決定する\n",
    "対称量子化なので、行列Xの最大値と最小値の差の２分の1を$p$-bitの数値範囲の最大値$q_{max}$でわる\n",
    "\n",
    "$q_{max} = 2^{(p-1)} - 1$\n",
    "\n",
    "$s = \\frac{max(X) - min(X)}{2q_{max}}$\n",
    "\n",
    "$X_{q} = s * \\text{clip}(\\text{round}(\\frac{X}{s}), -q_{max}, q_{max})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be40f54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warming up by no-quantized training...\n",
      "Epoch [1/5], Loss: 0.0736\n",
      "Epoch [2/5], Loss: 0.0658\n",
      "Epoch [3/5], Loss: 0.0576\n",
      "Epoch [4/5], Loss: 0.0513\n",
      "Epoch [5/5], Loss: 0.0448\n",
      "quantization aware training...\n",
      "Epoch [1/5], Loss: 0.0591\n",
      "Epoch [2/5], Loss: 0.0373\n",
      "Epoch [3/5], Loss: 0.0331\n",
      "Epoch [4/5], Loss: 0.0310\n",
      "Epoch [5/5], Loss: 0.0296\n",
      "Accuracy: 97.77%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class SymQuantSTE(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input: torch.Tensor, scale: torch.Tensor, num_bits: int):\n",
    "        if num_bits == 1:\n",
    "            s = scale.abs()\n",
    "            output = s * torch.sgn(input)\n",
    "        else:\n",
    "            s = scale.abs().clamp_min(1e-8)\n",
    "            qmax = 2 ** (num_bits - 1) - 1\n",
    "            q = torch.clamp(torch.round(input / s), -qmax, qmax)\n",
    "            output = q * s\n",
    "\n",
    "        # backward用に保存\n",
    "        ctx.save_for_backward(input, s)\n",
    "        ctx.num_bits = num_bits\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, s = ctx.saved_tensors   # forwardでsaveしたものを正しく取り出す\n",
    "        num_bits = ctx.num_bits\n",
    "        if num_bits == 1:\n",
    "            grad_input = torch.clamp(grad_output, -1, 1)\n",
    "        else:\n",
    "            qmax = 2 ** (num_bits - 1) - 1\n",
    "            mask = (input.abs() <= qmax * s).to(grad_output.dtype)\n",
    "            grad_input = grad_output * mask\n",
    "\n",
    "        return grad_input, None, None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SymQuantLinear(nn.Linear):\n",
    "    def __init__(self, in_features, out_features, bias=True, weight_bits=8, act_bits=None):\n",
    "        super().__init__(in_features, out_features, bias)\n",
    "        self.weight_bits = weight_bits\n",
    "        self.act_bits = act_bits\n",
    "\n",
    "    def forward(self, input):\n",
    "        # weight のスケール\n",
    "        if self.weight_bits == 1:\n",
    "            weight_scale = self.weight.abs().sum() / self.weight.numel()\n",
    "        else:\n",
    "            qmax_w = 2 ** (self.weight_bits - 1) - 1\n",
    "            weight_scale = (self.weight.max() - self.weight.min()) / (2 * qmax_w)\n",
    "\n",
    "        # activation のスケール\n",
    "        if self.act_bits is not None:\n",
    "            if self.act_bits == 1:\n",
    "                act_scale = input.abs().sum() / input.numel()\n",
    "            else:\n",
    "                qmax_a = 2 ** (self.act_bits - 1) - 1\n",
    "                act_scale = (input.max() - input.min()) / (2 * qmax_a)\n",
    "            input = SymQuantSTE.apply(input, act_scale, self.act_bits)\n",
    "\n",
    "        # quantized weight\n",
    "        w_q = SymQuantSTE.apply(self.weight, weight_scale, self.weight_bits)\n",
    "\n",
    "        return F.linear(input, w_q, self.bias)\n",
    "\n",
    "\n",
    "\n",
    "def replace_linear_with_quantizedlinear(module, weight_bits=8, act_bits=None):\n",
    "    for name, child in module.named_children():\n",
    "        # すでに QuantizedLinear ならスキップ\n",
    "        if isinstance(child, SymQuantLinear):\n",
    "            continue\n",
    "        if isinstance(child, nn.Linear):\n",
    "            qlinear = SymQuantLinear(\n",
    "                child.in_features,\n",
    "                child.out_features,\n",
    "                bias=(child.bias is not None),\n",
    "                weight_bits=weight_bits,\n",
    "                act_bits=act_bits\n",
    "            )\n",
    "            # 重みとバイアスをコピー\n",
    "            qlinear.weight.data.copy_(child.weight.data)\n",
    "            if child.bias is not None:\n",
    "                qlinear.bias.data.copy_(child.bias.data)\n",
    "            setattr(module, name, qlinear)\n",
    "        else:\n",
    "            replace_linear_with_quantizedlinear(child, weight_bits, act_bits)\n",
    "    return module\n",
    "\n",
    "# モデルの定義\n",
    "model = SimpleModel().to(device)\n",
    "# 通常学習\n",
    "print('warming up by no-quantized training...')\n",
    "model = train(model, lr=0.1, epochs=5, device=device)\n",
    "# Linear層をQuantizedLinearに置換\n",
    "model_q = replace_linear_with_quantizedlinear(model, weight_bits=4, act_bits=4)\n",
    "print('quantization aware training...')\n",
    "model_q = train(model_q, lr=1e-3, epochs=5, device=device)\n",
    "accuracy = compute_accuracy(model_q, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c513024",
   "metadata": {},
   "source": [
    "## ベクトル量子化の実行\n",
    "\n",
    "###  プロセス：k-meansで重み行列における列ベクトルをクラスタリング-->同じクラスの列ベクトルの重み共有-->重み共有しながら再度追加学習\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad9ca76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    k-means による行ベクトル量子化付き Linear 層\n",
    "    \"\"\"\n",
    "    def __init__(self, linear_layer: nn.Linear, n_clusters_percentage: int):\n",
    "        super().__init__()\n",
    "        self.in_features = linear_layer.in_features\n",
    "        self.out_features = linear_layer.out_features\n",
    "        # 行ベクトル数 = out_features\n",
    "        self.n_clusters = max(int(self.out_features * n_clusters_percentage * 0.01), 1)\n",
    "\n",
    "        # 元の重みとバイアスをコピー\n",
    "        W = linear_layer.weight.data.clone()  # (out_features, in_features) → 行ベクトル単位で扱う\n",
    "        self.bias = nn.Parameter(linear_layer.bias.data.clone() if linear_layer.bias is not None else None)\n",
    "\n",
    "        # k-means クラスタリング（各行を1サンプルとする）\n",
    "        kmeans = KMeans(n_clusters=self.n_clusters, n_init=10, random_state=0)\n",
    "        labels = kmeans.fit_predict(W.cpu().numpy())  # 各行ベクトルのクラスタ番号\n",
    "        centroids = torch.tensor(kmeans.cluster_centers_, dtype=W.dtype, device=W.device)  # (n_clusters, in_features)\n",
    "\n",
    "        # 各行の重みが属するクラスタインデックス\n",
    "        self.register_buffer(\"labels\", torch.tensor(labels, dtype=torch.long))\n",
    "        # 量子化されたクラスタ中心（学習可能にするなら Parameter にしても良い）\n",
    "        self.centroids = nn.Parameter(centroids)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # 各クラスタ中心から行ベクトルを再構築\n",
    "        W_q = self.centroids[self.labels]  # (out_features, in_features)\n",
    "        return torch.nn.functional.linear(x, W_q, self.bias)\n",
    "\n",
    "\n",
    "def replace_linear_with_vqlinear(model: nn.Module, n_clusters_percentage: int):\n",
    "    \"\"\"\n",
    "    モデル内の Linear 層を VQLinear に置換\n",
    "    \"\"\"\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            setattr(model, name, VQLinear(module, n_clusters_percentage))\n",
    "        else:\n",
    "            replace_linear_with_vqlinear(module, n_clusters_percentage)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92de6e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warming up by no-quantized training...\n",
      "Epoch [1/5], Loss: 0.0088\n",
      "Epoch [2/5], Loss: 0.0050\n",
      "Epoch [3/5], Loss: 0.0032\n",
      "Epoch [4/5], Loss: 0.0030\n",
      "Epoch [5/5], Loss: 0.0022\n",
      "quantization aware training...\n",
      "Epoch [1/5], Loss: 0.0020\n",
      "Epoch [2/5], Loss: 0.0016\n",
      "Epoch [3/5], Loss: 0.0014\n",
      "Epoch [4/5], Loss: 0.0013\n",
      "Epoch [5/5], Loss: 0.0012\n",
      "Accuracy: 98.21%\n"
     ]
    }
   ],
   "source": [
    "# モデルの定義\n",
    "model = SimpleModel().to(device)\n",
    "# 通常学習\n",
    "print('warming up by no-quantized training...')\n",
    "model = train(model, lr=0.1, epochs=5, device=device)\n",
    "# Linear層をVQLinearに置換 (元々の重みをn_clusters_percentage%の行数+インデックス分のメモリサイズに圧縮)\n",
    "model_q = replace_linear_with_vqlinear(model, n_clusters_percentage=10)\n",
    "print('quantization aware training...')\n",
    "model_q = train(model_q, lr=1e-3, epochs=5, device=device)\n",
    "accuracy = compute_accuracy(model_q, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ac0bfc",
   "metadata": {},
   "source": [
    "## 課題\n",
    "\n",
    "### ・ベクトル量子化とスカラー量子化におけるそれぞれのモデルサイズと精度のトレードオフを調べる\n",
    "### ・ベクトル量子化したセントロイドにスカラー量子化を適用することで更なる圧縮を試みる"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
