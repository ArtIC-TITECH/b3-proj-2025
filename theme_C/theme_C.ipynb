{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c115a3b",
   "metadata": {},
   "source": [
    "# テーマC：量子化の前処理としてのアダマール変換の評価\n",
    "[Open In Colab](https://colab.research.google.com/github/ArtIC-TITECH/b3-proj-2025/blob/main/theme_C/theme_C.ipynb)\n",
    "\n",
    "## モジュールの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1913299d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c2a50a",
   "metadata": {},
   "source": [
    "## MNISTのデータセット/精度評価関数の作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5279c324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 実行デバイスの設定\n",
    "device = 'cuda:2'\n",
    "\n",
    "# 普通のtransform\n",
    "transform_normal = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# テストデータには普通のtransformを使ってください\n",
    "transform_for_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform_normal) # モデルの学習に使うデータセット\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform_for_test) # モデルの評価に使うデータセット\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "def compute_accuracy(model, test_loader, device='cuda:0'):\n",
    "    model.eval()  # 評価モード\n",
    "    model.to(device)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images.to(device))\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels.to(device)).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy: {accuracy:.2f}%')\n",
    "    return accuracy\n",
    "\n",
    "def train(model, lr=0.05, epochs=5, device='cuda:0'):\n",
    "    # 損失関数と最適化手法の定義\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    model.to(device)\n",
    "    for epoch in range(epochs):\n",
    "        loss_sum = 0\n",
    "        for images, labels in train_loader:\n",
    "            # モデルの予測\n",
    "            outputs = model(images.to(device))\n",
    "\n",
    "            # 損失の計算\n",
    "            loss = criterion(outputs, labels.to(device))\n",
    "            loss_sum += loss.item()\n",
    "\n",
    "            # 勾配の初期化\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # バックプロパゲーション\n",
    "            loss.backward()\n",
    "\n",
    "            # オプティマイザの更新\n",
    "            optimizer.step()\n",
    "\n",
    "        # 損失を表示\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss_sum/len(train_loader):.4f}')\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e723368",
   "metadata": {},
   "source": [
    "\n",
    "## 通常モデルの学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "51c72c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.4222\n",
      "Epoch [2/10], Loss: 0.1989\n",
      "Epoch [3/10], Loss: 0.1546\n",
      "Epoch [4/10], Loss: 0.1305\n",
      "Epoch [5/10], Loss: 0.1130\n",
      "Epoch [6/10], Loss: 0.1019\n",
      "Epoch [7/10], Loss: 0.0921\n",
      "Epoch [8/10], Loss: 0.0854\n",
      "Epoch [9/10], Loss: 0.0808\n",
      "Epoch [10/10], Loss: 0.0727\n"
     ]
    }
   ],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self): # モデルのセットアップ\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 64)\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x): # モデルが行う処理\n",
    "        x = x.view(-1, 28 * 28)  # 28x28の画像を１次元に変換\n",
    "        x = self.fc1(x) \n",
    "        x = nn.ReLU()(x) # 活性化関数\n",
    "        x = self.fc2(x) \n",
    "        return x\n",
    "\n",
    "# モデルのインスタンスを作成\n",
    "model = SimpleModel().to(device)\n",
    "\n",
    "model = train(model, lr=0.1, epochs=10, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c5ee88",
   "metadata": {},
   "source": [
    "精度の確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a8317e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.83%\n"
     ]
    }
   ],
   "source": [
    "accuracy = compute_accuracy(model, test_loader, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a1d36f",
   "metadata": {},
   "source": [
    "## スカラー量子化（一様対称量子化）の実行\n",
    "\n",
    "###  プロセス：量子化層に変換-->量子化認識学習\n",
    "\n",
    "ここでは簡便に量子化パラメータをmin-maxスケーリングで決定する\n",
    "対称量子化なので、行列Xの最大値と最小値の差の２分の1を$p$-bitの数値範囲の最大値$q_{max}$でわる\n",
    "\n",
    "$q_{max} = 2^{(p-1)} - 1$\n",
    "\n",
    "$s = \\frac{max(X) - min(X)}{2q_{max}}$\n",
    "\n",
    "$X_{q} = s * \\text{clip}(\\text{round}(\\frac{X}{s}), -q_{max}, q_{max})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df401a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warming up by no-quantized training...\n",
      "Epoch [1/5], Loss: 0.4251\n",
      "Epoch [2/5], Loss: 0.2056\n",
      "Epoch [3/5], Loss: 0.1570\n",
      "Epoch [4/5], Loss: 0.1307\n",
      "Epoch [5/5], Loss: 0.1166\n",
      "quantization aware training...\n",
      "Epoch [1/10], Loss: 1.5707\n",
      "Epoch [2/10], Loss: 0.9505\n",
      "Epoch [3/10], Loss: 0.7470\n",
      "Epoch [4/10], Loss: 0.6746\n",
      "Epoch [5/10], Loss: 0.6514\n",
      "Epoch [6/10], Loss: 0.6492\n",
      "Epoch [7/10], Loss: 0.5975\n",
      "Epoch [8/10], Loss: 0.5278\n",
      "Epoch [9/10], Loss: 0.5058\n",
      "Epoch [10/10], Loss: 0.5199\n",
      "Accuracy: 83.57%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class SymQuantSTE(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input: torch.Tensor, scale: torch.Tensor, num_bits: int):\n",
    "        if num_bits == 1:\n",
    "            s = scale.abs()\n",
    "            output = s * torch.sgn(input)\n",
    "        else:\n",
    "            s = scale.abs().clamp_min(1e-8)\n",
    "            qmax = 2 ** (num_bits - 1) - 1\n",
    "            q = torch.clamp(torch.round(input / s), -qmax, qmax)\n",
    "            output = q * s\n",
    "\n",
    "        # backward用に保存\n",
    "        ctx.save_for_backward(input, s)\n",
    "        ctx.num_bits = num_bits\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, s = ctx.saved_tensors   # forwardでsaveしたものを正しく取り出す\n",
    "        num_bits = ctx.num_bits\n",
    "        if num_bits == 1:\n",
    "            grad_input = torch.clamp(grad_output, -1, 1)\n",
    "        else:\n",
    "            qmax = 2 ** (num_bits - 1) - 1\n",
    "            mask = (input.abs() <= qmax * s).to(grad_output.dtype)\n",
    "            grad_input = grad_output * mask\n",
    "\n",
    "        return grad_input, None, None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SymQuantLinear(nn.Linear):\n",
    "    def __init__(self, in_features, out_features, bias=True, weight_bits=8, act_bits=None):\n",
    "        super().__init__(in_features, out_features, bias)\n",
    "        self.weight_bits = weight_bits\n",
    "        self.act_bits = act_bits\n",
    "\n",
    "    def forward(self, input):\n",
    "        # weight のスケール\n",
    "        if self.weight_bits == 1:\n",
    "            weight_scale = self.weight.abs().sum() / self.weight.numel()\n",
    "        else:\n",
    "            qmax_w = 2 ** (self.weight_bits - 1) - 1\n",
    "            weight_scale = (self.weight.max() - self.weight.min()) / (2 * qmax_w)\n",
    "\n",
    "        # activation のスケール\n",
    "        if self.act_bits is not None:\n",
    "            if self.act_bits == 1:\n",
    "                act_scale = input.abs().sum() / input.numel()\n",
    "            else:\n",
    "                qmax_a = 2 ** (self.act_bits - 1) - 1\n",
    "                act_scale = (input.max() - input.min()) / (2 * qmax_a)\n",
    "            input = SymQuantSTE.apply(input, act_scale, self.act_bits)\n",
    "\n",
    "        # quantized weight\n",
    "        w_q = SymQuantSTE.apply(self.weight, weight_scale, self.weight_bits)\n",
    "\n",
    "        return F.linear(input, w_q, self.bias)\n",
    "\n",
    "\n",
    "\n",
    "def replace_linear_with_quantizedlinear(module, weight_bits=8, act_bits=None):\n",
    "    for name, child in module.named_children():\n",
    "        # すでに QuantizedLinear ならスキップ\n",
    "        if isinstance(child, SymQuantLinear):\n",
    "            continue\n",
    "        if isinstance(child, nn.Linear):\n",
    "            qlinear = SymQuantLinear(\n",
    "                child.in_features,\n",
    "                child.out_features,\n",
    "                bias=(child.bias is not None),\n",
    "                weight_bits=weight_bits,\n",
    "                act_bits=act_bits\n",
    "            )\n",
    "            # 重みとバイアスをコピー\n",
    "            qlinear.weight.data.copy_(child.weight.data)\n",
    "            if child.bias is not None:\n",
    "                qlinear.bias.data.copy_(child.bias.data)\n",
    "            setattr(module, name, qlinear)\n",
    "        else:\n",
    "            replace_linear_with_quantizedlinear(child, weight_bits, act_bits)\n",
    "    return module\n",
    "\n",
    "# モデルのインスタンスを作成\n",
    "model = SimpleModel().to(device)\n",
    "# 通常学習\n",
    "print('warming up by no-quantized training...')\n",
    "model = train(model, lr=0.1, epochs=5, device=device)\n",
    "# Linear層をQuantizedLinearに置換\n",
    "model_q = replace_linear_with_quantizedlinear(model, weight_bits=1, act_bits=1)\n",
    "print('quantization aware training...')\n",
    "model_q = train(model_q, lr=1e-2, epochs=10, device=device)\n",
    "accuracy = compute_accuracy(model_q, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed09bb8e",
   "metadata": {},
   "source": [
    "## アダマール変換付きスカラー量子化（一様対称量子化）の実行\n",
    "\n",
    "###  プロセス：アダマール変換付き量子化層に変換-->量子化認識学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1a2738f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warming up by no-quantized training...\n",
      "Epoch [1/5], Loss: 0.5063\n",
      "Epoch [2/5], Loss: 0.2324\n",
      "Epoch [3/5], Loss: 0.1804\n",
      "Epoch [4/5], Loss: 0.1535\n",
      "Epoch [5/5], Loss: 0.1358\n",
      "hadamard transformation and quantization aware training...\n",
      "Epoch [1/10], Loss: 1.1305\n",
      "Epoch [2/10], Loss: 0.6893\n",
      "Epoch [3/10], Loss: 0.5900\n",
      "Epoch [4/10], Loss: 0.5412\n",
      "Epoch [5/10], Loss: 0.5137\n",
      "Epoch [6/10], Loss: 0.4973\n",
      "Epoch [7/10], Loss: 0.4788\n",
      "Epoch [8/10], Loss: 0.4680\n",
      "Epoch [9/10], Loss: 0.4615\n",
      "Epoch [10/10], Loss: 0.4535\n",
      "Accuracy: 85.43%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "class SymQuantSTE(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input: torch.Tensor, scale: torch.Tensor, num_bits: int):\n",
    "        if num_bits == 1:\n",
    "            s = scale.abs()\n",
    "            output = s * torch.sgn(input)\n",
    "        else:\n",
    "            s = scale.abs().clamp_min(1e-8)\n",
    "            qmax = 2 ** (num_bits - 1) - 1\n",
    "            q = torch.clamp(torch.round(input / s), -qmax, qmax)\n",
    "            output = q * s\n",
    "\n",
    "        ctx.save_for_backward(input, s)\n",
    "        ctx.num_bits = num_bits\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, s = ctx.saved_tensors\n",
    "        num_bits = ctx.num_bits\n",
    "        if num_bits == 1:\n",
    "            mask = (input.abs() <= s).to(grad_output.dtype)\n",
    "            grad_input = torch.clamp(grad_output, -1, 1)\n",
    "        else:\n",
    "            qmax = 2 ** (num_bits - 1) - 1\n",
    "            mask = (input.abs() <= qmax * s).to(grad_output.dtype)\n",
    "            grad_input = grad_output * mask\n",
    "\n",
    "        return grad_input, None, None\n",
    "\n",
    "\n",
    "def hadamard_matrix(n: int, device=None, dtype=None) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    n次のアダマール行列を生成 (nは2のべき乗)\n",
    "    \"\"\"\n",
    "    if n == 1:\n",
    "        return torch.tensor([[1.0]], device=device, dtype=dtype)\n",
    "    H = hadamard_matrix(n // 2, device=device, dtype=dtype)\n",
    "    top = torch.cat([H, H], dim=1)\n",
    "    bottom = torch.cat([H, -H], dim=1)\n",
    "    return torch.cat([top, bottom], dim=0)\n",
    "\n",
    "\n",
    "def clipped_hadamard(n: int, device=None, dtype=None) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    n次元入力に対応するクリップ版アダマール行列を返す\n",
    "    - 入力次元nに対して、2^k >= n を探す\n",
    "    - 2^k 次のアダマール行列を作り、左上の n×n を取り出す\n",
    "    \"\"\"\n",
    "    k = (n - 1).bit_length()  # ceil(log2(n))\n",
    "    H_full = hadamard_matrix(2**k, device=device, dtype=dtype)\n",
    "    H_clip = H_full[:n, :n]\n",
    "    return H_clip / (n**0.5)  # 正規化\n",
    "\n",
    "\n",
    "class HadamardSymQuantLinear(nn.Linear):\n",
    "    def __init__(self, in_features, out_features, bias=True, weight_bits=8, act_bits=None):\n",
    "        super().__init__(in_features, out_features, bias)\n",
    "        self.weight_bits = weight_bits\n",
    "        self.act_bits = act_bits\n",
    "        # 固定Hadamard行列をbufferに登録（学習しない）\n",
    "        H = clipped_hadamard(in_features)\n",
    "        self.register_buffer(\"H\", H)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # --- アダマール変換 ---\n",
    "        input = input @ self.H.T   # (batch, in_features) × (in_features, in_features)\n",
    "\n",
    "        # --- weight のスケール ---\n",
    "        if self.weight_bits is not None:\n",
    "            if self.weight_bits == 1:\n",
    "                weight_scale = self.weight.abs().sum() / self.weight.numel()\n",
    "            else:\n",
    "                qmax_w = 2 ** (self.weight_bits - 1) - 1\n",
    "                weight_scale = (self.weight.max() - self.weight.min()) / (2 * qmax_w)\n",
    "            # --- quantized weight ---\n",
    "            w_q = SymQuantSTE.apply(self.weight, weight_scale, self.weight_bits)\n",
    "        else:\n",
    "            weight_scale = None\n",
    "            w_q = self.weight\n",
    "\n",
    "        # --- activation のスケール ---\n",
    "        if self.act_bits is not None:\n",
    "            if self.act_bits == 1:\n",
    "                act_scale = input.abs().sum() / input.numel()\n",
    "            else:\n",
    "                qmax_a = 2 ** (self.act_bits - 1) - 1\n",
    "                act_scale = (input.max() - input.min()) / (2 * qmax_a)\n",
    "            input = SymQuantSTE.apply(input, act_scale, self.act_bits)\n",
    "\n",
    "        return F.linear(input, w_q, self.bias)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def replace_linear_with_hadamard_quantizedlinear(module, weight_bits=8, act_bits=None):\n",
    "    for name, child in module.named_children():\n",
    "        # すでに QuantizedLinear ならスキップ\n",
    "        if isinstance(child, HadamardSymQuantLinear):\n",
    "            qlinear = HadamardSymQuantLinear(\n",
    "                child.in_features,\n",
    "                child.out_features,\n",
    "                bias=(child.bias is not None),\n",
    "                weight_bits=weight_bits,\n",
    "                act_bits=act_bits\n",
    "            )\n",
    "            # 重みとバイアスをコピー\n",
    "            qlinear.weight.data.copy_(child.weight.data)\n",
    "            if child.bias is not None:\n",
    "                qlinear.bias.data.copy_(child.bias.data)\n",
    "            setattr(module, name, qlinear)\n",
    "        if isinstance(child, nn.Linear):\n",
    "            qlinear = HadamardSymQuantLinear(\n",
    "                child.in_features,\n",
    "                child.out_features,\n",
    "                bias=(child.bias is not None),\n",
    "                weight_bits=weight_bits,\n",
    "                act_bits=act_bits\n",
    "            )\n",
    "            # 重みとバイアスをコピー\n",
    "            qlinear.weight.data.copy_(child.weight.data)\n",
    "            if child.bias is not None:\n",
    "                qlinear.bias.data.copy_(child.bias.data)\n",
    "            setattr(module, name, qlinear)\n",
    "        else:\n",
    "            replace_linear_with_hadamard_quantizedlinear(child, weight_bits, act_bits)\n",
    "    return module\n",
    "\n",
    "# モデルのインスタンスを作成\n",
    "model = SimpleModel().to(device)\n",
    "\n",
    "# 通常学習\n",
    "model = replace_linear_with_hadamard_quantizedlinear(model, weight_bits=None, act_bits=None)\n",
    "print('warming up by no-quantized training...')\n",
    "model = train(model, lr=0.1, epochs=5, device=device)\n",
    "# Linear層をHadamardQuantizedLinearに置換\n",
    "model_q = replace_linear_with_hadamard_quantizedlinear(model, weight_bits=1, act_bits=1)\n",
    "print('hadamard transformation and quantization aware training...')\n",
    "model_q = train(model_q, lr=1e-2, epochs=10, device=device)\n",
    "accuracy = compute_accuracy(model_q, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f863b87a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warming up by no-quantized training...\n",
      "Epoch [1/5], Loss: 0.4199\n",
      "Epoch [2/5], Loss: 0.1933\n",
      "Epoch [3/5], Loss: 0.1486\n",
      "Epoch [4/5], Loss: 0.1263\n",
      "Epoch [5/5], Loss: 0.1115\n",
      "randomized hadamard transformation and quantization aware training...\n",
      "Epoch [1/10], Loss: 1.0163\n",
      "Epoch [2/10], Loss: 0.6615\n",
      "Epoch [3/10], Loss: 0.5934\n",
      "Epoch [4/10], Loss: 0.5739\n",
      "Epoch [5/10], Loss: 0.5602\n",
      "Epoch [6/10], Loss: 0.5377\n",
      "Epoch [7/10], Loss: 0.5141\n",
      "Epoch [8/10], Loss: 0.5040\n",
      "Epoch [9/10], Loss: 0.4932\n",
      "Epoch [10/10], Loss: 0.4918\n",
      "Accuracy: 85.01%\n"
     ]
    }
   ],
   "source": [
    "class RandomizedHadamardSymQuantLinear(nn.Linear):\n",
    "    def __init__(self, in_features, out_features, bias=True, weight_bits=8, act_bits=None):\n",
    "        super().__init__(in_features, out_features, bias)\n",
    "        self.weight_bits = weight_bits\n",
    "        self.act_bits = act_bits\n",
    "        \n",
    "        # Hadamard 行列\n",
    "        H = clipped_hadamard(in_features)\n",
    "        # ランダム ±1 符号\n",
    "        D = torch.randint(0, 2, (in_features,), dtype=torch.float32) * 2 - 1\n",
    "        H_rht = H * D.unsqueeze(0)  # 列ごとに符号を掛ける\n",
    "        self.register_buffer(\"H_rht\", H_rht)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # --- Randomized Hadamard Transform ---\n",
    "        input = input @ self.H_rht.T\n",
    "\n",
    "        # --- weight のスケール ---\n",
    "        if self.weight_bits is not None:\n",
    "            if self.weight_bits == 1:\n",
    "                weight_scale = self.weight.abs().sum() / self.weight.numel()\n",
    "            else:\n",
    "                qmax_w = 2 ** (self.weight_bits - 1) - 1\n",
    "                weight_scale = (self.weight.max() - self.weight.min()) / (2 * qmax_w)\n",
    "            # --- quantized weight ---\n",
    "            w_q = SymQuantSTE.apply(self.weight, weight_scale, self.weight_bits)\n",
    "        else:\n",
    "            w_q = self.weight\n",
    "\n",
    "        # --- activation のスケール ---\n",
    "        if self.act_bits is not None:\n",
    "            if self.act_bits == 1:\n",
    "                act_scale = input.abs().sum() / input.numel()\n",
    "            else:\n",
    "                qmax_a = 2 ** (self.act_bits - 1) - 1\n",
    "                act_scale = (input.max() - input.min()) / (2 * qmax_a)\n",
    "            input = SymQuantSTE.apply(input, act_scale, self.act_bits)\n",
    "\n",
    "        return F.linear(input, w_q, self.bias)\n",
    "    \n",
    "\n",
    "def replace_linear_with_randomized_hadamard_quantizedlinear(module, weight_bits=8, act_bits=None):\n",
    "    for name, child in module.named_children():\n",
    "        if isinstance(child, RandomizedHadamardSymQuantLinear):\n",
    "            qlinear = RandomizedHadamardSymQuantLinear(\n",
    "                child.in_features,\n",
    "                child.out_features,\n",
    "                bias=(child.bias is not None),\n",
    "                weight_bits=weight_bits,\n",
    "                act_bits=act_bits\n",
    "            )\n",
    "            # 重みとバイアスをコピー\n",
    "            qlinear.weight.data.copy_(child.weight.data)\n",
    "            if child.bias is not None:\n",
    "                qlinear.bias.data.copy_(child.bias.data)\n",
    "            setattr(module, name, qlinear)\n",
    "        if isinstance(child, nn.Linear):\n",
    "            qlinear = RandomizedHadamardSymQuantLinear(\n",
    "                child.in_features,\n",
    "                child.out_features,\n",
    "                bias=(child.bias is not None),\n",
    "                weight_bits=weight_bits,\n",
    "                act_bits=act_bits\n",
    "            )\n",
    "            # 重みとバイアスをコピー\n",
    "            qlinear.weight.data.copy_(child.weight.data)\n",
    "            if child.bias is not None:\n",
    "                qlinear.bias.data.copy_(child.bias.data)\n",
    "            setattr(module, name, qlinear)\n",
    "        else:\n",
    "            replace_linear_with_randomized_hadamard_quantizedlinear(child, weight_bits, act_bits)\n",
    "    return module\n",
    "\n",
    "\n",
    "# モデルのインスタンスを作成\n",
    "model = SimpleModel().to(device)\n",
    "# 通常学習\n",
    "print('warming up by no-quantized training...')\n",
    "model = replace_linear_with_randomized_hadamard_quantizedlinear(model, weight_bits=None, act_bits=None)\n",
    "model = train(model, lr=0.1, epochs=5, device=device)\n",
    "# Linear層をHadamardQuantizedLinearに置換\n",
    "model_q = replace_linear_with_randomized_hadamard_quantizedlinear(model, weight_bits=1, act_bits=1)\n",
    "print('randomized hadamard transformation and quantization aware training...')\n",
    "model_q = train(model_q, lr=1e-2, epochs=10, device=device)\n",
    "accuracy = compute_accuracy(model_q, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e9e7eb",
   "metadata": {},
   "source": [
    "## モデルサイズの確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff800602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_model_size(model: nn.Module):\n",
    "    total_size = 0\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # 通常の Linear 層のサイズ (float32)\n",
    "            total_size += module.weight.numel() * 4  # float32 として計算\n",
    "            if module.bias is not None:\n",
    "                total_size += module.bias.numel() * 4  # float32 として計算\n",
    "        elif isinstance(module, (SymQuantLinear, HadamardSymQuantLinear, RandomizedHadamardSymQuantLinear)):\n",
    "            if module.weight_bits is None:\n",
    "                module.weight_bits = 32  # float32として計算\n",
    "            # weight のサイズ\n",
    "            total_size += module.weight.numel() * module.weight_bits/8  \n",
    "            # bias のサイズ\n",
    "            if module.bias is not None:\n",
    "                total_size += module.bias.numel() * 4  # float32 として計算\n",
    "    return total_size / (1024 * 1024)  # MB単位で返す\n",
    "\n",
    "model_size = compute_model_size(model_q)\n",
    "print(f'Model size: {model_size:.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cf083a",
   "metadata": {},
   "source": [
    "## 課題\n",
    "### ・普通に量子化した場合、アダマール変換を入れて量子化した場合、ランダマイズドアダマール変換を入れて量子化した場合の精度をそれぞれ比較する（低ビットで比較しないとわからないかも）\n",
    "### ・実験結果について考察する"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
